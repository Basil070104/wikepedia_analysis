{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFGD6Vv-4Mor"
      },
      "outputs": [],
      "source": [
        "! pip install polars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2QPNzT3-07T"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import datetime as dt\n",
        "\n",
        "df = pl.DataFrame(\n",
        "    {\n",
        "        \"name\": [\"Alice Archer\", \"Ben Brown\", \"Chloe Cooper\", \"Daniel Donovan\"],\n",
        "        \"birthdate\": [\n",
        "            dt.date(1997, 1, 10),\n",
        "            dt.date(1985, 2, 15),\n",
        "            dt.date(1983, 3, 22),\n",
        "            dt.date(1981, 4, 30),\n",
        "        ],\n",
        "        \"weight\": [57.9, 72.5, 53.6, 83.1],  # (kg)\n",
        "        \"height\": [1.56, 1.77, 1.65, 1.75],  # (m)\n",
        "    }\n",
        ")\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9PYwcpkywOh"
      },
      "source": [
        "Lazy Loading of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSPfx7Ou_ZD4"
      },
      "outputs": [],
      "source": [
        "df = pl.scan_parquet(\"https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdata.ljrobins.com%2Ffiles%2Fpage.parquet&data=05%7C02%7Cbkhwaja%40purdue.edu%7C37993b20219243afa89c08ddc561d36f%7C4130bd397c53419cb1e58758d6d63f21%7C1%7C0%7C638883746144100445%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=FaRLG%2F0XFctogRNcYmjtH3lE6D7cNlry7fojrlA0dos%3D&reserved=0\")\n",
        "# print(df)\n",
        "print(df.explain())\n",
        "links = pl.scan_parquet(\"https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdata.ljrobins.com%2Ffiles%2Flinkpairs.parquet&data=05%7C02%7Cbkhwaja%40purdue.edu%7C37993b20219243afa89c08ddc561d36f%7C4130bd397c53419cb1e58758d6d63f21%7C1%7C0%7C638883746144130439%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=Uk3kXId438qv6o%2FN2Ax4zYlPC00mW6ZD2KXR6qfc9bU%3D&reserved=0\")\n",
        "print(links.explain())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCKYvODsyttb"
      },
      "outputs": [],
      "source": [
        "page = df.collect()\n",
        "linkPairs = links.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI8LbDdI0kwC"
      },
      "source": [
        "Graph Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KTQ8-TsznSc"
      },
      "outputs": [],
      "source": [
        "print(page)\n",
        "print(linkPairs)\n",
        "# edge definition -> linkPairs\n",
        "# node definition -> page_title (get from pages)\n",
        "\n",
        "class GraphNode:\n",
        "\n",
        "  def __init__(self, nodeId=0, nodeName=\"\") -> None:\n",
        "    self.nodeId = nodeId\n",
        "    self.nodeName = nodeName\n",
        "    self.edges = []\n",
        "\n",
        "  def addEdge(self, otherId) -> None:\n",
        "    self.edges.append(otherId)\n",
        "\n",
        "  def __str__(self) -> str:\n",
        "      return f\"Graph node {self.nodeId} with the name {self.nodeName} has these connection {self.edges}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8Fc5JKV1mY_"
      },
      "outputs": [],
      "source": [
        "# create graph dictionary\n",
        "graph = {}\n",
        "for row in page.iter_rows():\n",
        "  id, name = row\n",
        "  node = GraphNode(nodeId=id, nodeName=name)\n",
        "  graph.update({id : node})\n",
        "\n",
        "# create edges\n",
        "for row in linkPairs.iter_rows():\n",
        "  plFrom, plTo = row\n",
        "  node = graph[plFrom]\n",
        "  node.addEdge(otherId=plTo)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw4Crs3cbF7X"
      },
      "outputs": [],
      "source": [
        "# del page\n",
        "# del linkPairs\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drMUUnXq8VVg"
      },
      "source": [
        "### Question :\n",
        "\n",
        "How to effectively create study plan for students using Wikipedia?\n",
        "\n",
        "Further Questions:\n",
        "Finding other people that are reseraching the same topics as you"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SMsPGi06hxp"
      },
      "source": [
        "Create an Example User Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZi3QQwH6Xys"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class User:\n",
        "\n",
        "  def __init__(self, name=\"\") -> None:\n",
        "    self.name = name\n",
        "    self.visitedPages = {} # holds the ids visited and its frequency of times visited\n",
        "    self.recommendedPages = [] # id of the wiki pages\n",
        "\n",
        "  def addPage(self, id) -> None:\n",
        "    if id in self.visitedPages:\n",
        "      self.visitedPages[id] += 1\n",
        "    else:\n",
        "      self.visitedPages.update({id : 1})\n",
        "\n",
        "  def addRecommendedPapges(self, id) -> None:\n",
        "    self.recommendedPages.append(id)\n",
        "\n",
        "  def __str__(self) -> str:\n",
        "    return f\"{self.name} has visited these pages {self.visitedPages}\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrFXhv-z9AYR"
      },
      "outputs": [],
      "source": [
        "# Create a recommendation algorithm for the pages visited by the user\n",
        "# possible articles in the vicinity of the pages visited by the user -> BFS\n",
        "user = User(name=\"Basil Khwaja\")\n",
        "random_idx = np.random.randint(0, len(graph), size=10)\n",
        "id_keys = list(graph.keys())\n",
        "for idx in random_idx:\n",
        "  user.addPage(id_keys[idx])\n",
        "\n",
        "# print(user)\n",
        "\n",
        "def recommendArticles(user):\n",
        "\n",
        "  visitedPages = user.visitedPages\n",
        "  visitedKeys = list(user.visitedPages.keys())\n",
        "  print(f\"Pages visited by {user.name}\")\n",
        "  for key in visitedKeys:\n",
        "    node = graph.get(key)\n",
        "    print(node.nodeName)\n",
        "\n",
        "  print(\"\\n\")\n",
        "\n",
        "  cluster = {}\n",
        "  maxDistance = 2 # changes to see different recommendation results\n",
        "\n",
        "  def bfs(start_id, max_distance) -> None:\n",
        "      queue = [(start_id, 0)]  # (node_id, distance)\n",
        "\n",
        "      while queue:\n",
        "          current_id, distance = queue.pop(0)\n",
        "\n",
        "          # if current_id not in visitedKeys:\n",
        "          if current_id in cluster:\n",
        "            cluster[current_id] += 1\n",
        "          else:\n",
        "            cluster.update({current_id : 1})\n",
        "\n",
        "          if distance < max_distance:\n",
        "              node = graph.get(current_id)\n",
        "              if node:\n",
        "                  for neighbor in node.edges:\n",
        "                      queue.append((neighbor, distance + 1))\n",
        "\n",
        "  for key, value in visitedPages.items():\n",
        "    bfs(key, maxDistance)\n",
        "\n",
        "  # print(cluster)\n",
        "\n",
        "  # overlap in graph is more of importance that means there is a shared article of importance\n",
        "  # sort by value of descencing overlap in graph\n",
        "  clusterDesc = dict(sorted(cluster.items(), key=lambda item: item[1], reverse=True))\n",
        "  # print(clusterDesc)\n",
        "\n",
        "  # Recommend the user these articles to read\n",
        "  recommendList = list(clusterDesc.keys())\n",
        "  # print(f\"Recommend {user.name} these next articles to read : \")\n",
        "  # for i in range(5):\n",
        "  #   node = graph.get(recommendList[i])\n",
        "  #   print(node.nodeName)\n",
        "\n",
        "  return recommendList\n",
        "\n",
        "recommendList = recommendArticles(user)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UspHxQqNHd5"
      },
      "outputs": [],
      "source": [
        "!pip install Wikipedia-API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8088ec0"
      },
      "outputs": [],
      "source": [
        "import wikipediaapi\n",
        "\n",
        "def get_wiki_summary(articleTitle):\n",
        "    wiki = wikipediaapi.Wikipedia(user_agent='StudyPlan (khwajabasil@gmail.com)', language='en')\n",
        "    page = wiki.page(articleTitle)\n",
        "\n",
        "    if page.exists():\n",
        "        return page.summary\n",
        "    else:\n",
        "        return f\"Could not find a summary for '{articleTitle}' on Wikipedia.\"\n",
        "\n",
        "recommendedTitles = []\n",
        "for i in range(5):\n",
        "  node = graph.get(recommendList[i])\n",
        "  recommendedTitles.append(node.nodeName)\n",
        "\n",
        "print(\"Summary of Recommended Articles to look into\")\n",
        "print(\"=\"*40)\n",
        "for recommended in recommendedTitles:\n",
        "  summary = get_wiki_summary(recommended)\n",
        "  print(f\"\\n--- {recommended} ---\")\n",
        "  print(summary)\n",
        "  print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i0xegaz5E3R"
      },
      "outputs": [],
      "source": [
        "# !pip install nx-cugraph-cu11 --extra-index-url https://pypi.nvidia.com\n",
        "!nvidia-smi\n",
        "# !pip install nx-cugraph-cu12 --extra-index-url https://pypi.anaconda.org/rapidsai-wheels-nightly/simple\n",
        "# !pip uninstall cupy-cuda11x cupy-cuda12x\n",
        "! pip install cupy-cuda12x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWkBCpdDLYVb"
      },
      "source": [
        "### Grab the Subgraph of the User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c7399e1"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import cupy\n",
        "print(cupy.__version__)\n",
        "import nx_cugraph as nxcg\n",
        "\n",
        "\n",
        "# Create a new directed graph for the subgraph\n",
        "subgraph = nx.Graph()\n",
        "\n",
        "maxDistance = 1\n",
        "visitedPages = user.visitedPages\n",
        "\n",
        "for pageId in visitedPages.keys():\n",
        "    if pageId in graph:\n",
        "        node = graph[pageId]\n",
        "        subgraph.add_node(node.nodeId, name=node.nodeName, label=node.nodeName, type='visited')\n",
        "\n",
        "nodesToExplore = list(visitedPages.keys())\n",
        "visitedInBfs = set(visitedPages.keys())\n",
        "\n",
        "for start_node_id in nodesToExplore:\n",
        "    queue = [(start_node_id, 0)]\n",
        "\n",
        "    while queue:\n",
        "        current_id, distance = queue.pop(0)\n",
        "\n",
        "        if current_id in graph:\n",
        "            current_node = graph[current_id]\n",
        "\n",
        "            if current_id not in subgraph.nodes():\n",
        "                 subgraph.add_node(current_node.nodeId, name=current_node.nodeName, label=current_node.nodeName, type='neighbor')\n",
        "\n",
        "            if distance < maxDistance:\n",
        "                for neighbor_id in current_node.edges:\n",
        "                    if neighbor_id in graph:\n",
        "                         neighbor_node = graph[neighbor_id]\n",
        "                         if neighbor_id not in subgraph.nodes():\n",
        "                              subgraph.add_node(neighbor_node.nodeId, name=neighbor_node.nodeName, label=neighbor_node.nodeName, type='neighbor')\n",
        "\n",
        "                         subgraph.add_edge(current_id, neighbor_id)\n",
        "\n",
        "                         if neighbor_id not in visitedInBfs:\n",
        "                             visitedInBfs.add(neighbor_id)\n",
        "                             queue.append((neighbor_id, distance + 1))\n",
        "\n",
        "\n",
        "print(f\"Created a subgraph with {subgraph.number_of_nodes()} nodes and {subgraph.number_of_edges()} edges.\")\n",
        "\n",
        "# convert to cuGraph for better GPU optimization becuase my CPU is cooked :(\n",
        "\n",
        "nxcgSubgraph = nxcg.from_networkx(subgraph)\n",
        "fq2Layout = nxcg.forceatlas2_layout(nxcgSubgraph, max_iter=500)\n",
        "# Fix: Iterate through the dictionary items directly\n",
        "pos = {int(node_id): (float(coords[0]), float(coords[1])) for node_id, coords in fq2Layout.items()}\n",
        "\n",
        "# Scale out the plot by increasing figure size\n",
        "plt.figure(figsize=(20, 20)) # You can adjust the size as needed\n",
        "\n",
        "nx.draw(subgraph, pos=pos, with_labels=True, node_size=300, font_size=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZrPvyCvInvx"
      },
      "outputs": [],
      "source": [
        "import cupy\n",
        "print(cupy.__version__)\n",
        "import nx_cugraph as nxcg\n",
        "\n",
        "print(\"GPU available:\", cupy.cuda.is_available())\n",
        "nxcgSubgraph = nxcg.from_networkx(subgraph)\n",
        "fq2Layout = nxcg.forceatlas2_layout(nxcgSubgraph, max_iter=500)\n",
        "pos = {int(node_id): (float(coords[0]), float(coords[1])) for node_id, coords in fq2Layout.items()}\n",
        "nx.draw(subgraph, pos=pos, with_labels=True, node_size=20, font_size=5)\n",
        "plt.figure(figsize=(100,100))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KRVElTwNPhs"
      },
      "outputs": [],
      "source": [
        "! pip install pyvis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sepl08fGjOxW"
      },
      "source": [
        "OMG NEVER LET THE GRAPH GET OUT OF CONTROL\n",
        "---\n",
        "Changing the max distance from 2 -> 1 helped render the graph SO much better\n",
        "---\n",
        "At most rn form clusters of one because of the amount of connections that exist between the wikepedia pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvW-EZM-gK8q"
      },
      "outputs": [],
      "source": [
        "from pyvis.network import Network\n",
        "from google.colab import files\n",
        "\n",
        "nt = Network(height=\"750px\", width=\"70%\", bgcolor=\"#222222\", font_color=\"white\")\n",
        "nt.from_nx(subgraph)\n",
        "# nt.toggle_physics(True)\n",
        "nt.show_buttons(filter_=['physics'])\n",
        "nt.save_graph(\"subgraph.html\")\n",
        "# files.download(\"graph.html\")\n",
        "# nt.show(\"graph.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXXBsMwktT2k"
      },
      "source": [
        "### Simulate Multiple Users\n",
        "\n",
        "Learn about the user more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPCWPgnhtTSq"
      },
      "outputs": [],
      "source": [
        "class Platform:\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "     self.users = []\n",
        "\n",
        "  def addUser(self,user):\n",
        "    self.users.append(user)\n",
        "\n",
        "  def __str__(self) -> str:\n",
        "     return f\"{len(self.users)} Users exist on the platform\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa2851c2"
      },
      "outputs": [],
      "source": [
        "platform = Platform()\n",
        "numberOfUsers = 5\n",
        "for i in range(0, numberOfUsers):\n",
        "  user = User(f\"User {i}\")\n",
        "  platform.addUser(user)\n",
        "\n",
        "print(platform)\n",
        "\n",
        "for user in platform.users:\n",
        "  random_idx = np.random.randint(0, len(graph), size=10)\n",
        "  id_keys = list(graph.keys())\n",
        "  for idx in random_idx:\n",
        "    user.addPage(id_keys[idx])\n",
        "\n",
        "  recommnedations = recommendArticles(user)\n",
        "  # print(recommnedations)\n",
        "\n",
        "  user.recommendedPages = recommnedations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNUaCcLmZ_G4"
      },
      "source": [
        "Embeddings were cooking my RAM :(\n",
        "\n",
        "We can more experiments on the data given more time and better resources. My poor collab about to start crying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e78282a9"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "usersRecommendedTitles = {user.name: user.recommendedPages for user in platform.users}\n",
        "\n",
        "# Load a pre-trained sentence transformer model\n",
        "# Ensure the model is on the GPU if available\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
        "\n",
        "userNames = list(usersRecommendedTitles.keys())\n",
        "nums = len(userNames)\n",
        "similarityScores = {}\n",
        "\n",
        "def convertIdtoNames(idList):\n",
        "  newTitles = []\n",
        "  for id in idList:\n",
        "    node = graph.get(id)\n",
        "    newTitles.append(node.nodeName)\n",
        "\n",
        "  return newTitles\n",
        "\n",
        "for i in range(len(userNames)):\n",
        "  for j in range(i + 1, len(userNames)):\n",
        "    user1 = userNames[i]\n",
        "    user2 = userNames[j]\n",
        "\n",
        "    id1 = usersRecommendedTitles[user1]\n",
        "    id2 = usersRecommendedTitles[user2]\n",
        "\n",
        "    titles1 = convertIdtoNames(id1)\n",
        "    titles2 = convertIdtoNames(id2)\n",
        "\n",
        "    if not titles1 or not titles2:\n",
        "        continue  # Skip if either user has no valid titles\n",
        "\n",
        "    # Encode titles into embeddings (NO precision arg)\n",
        "    embeddings = model.encode(titles1, convert_to_numpy=True)\n",
        "    avg = np.mean(embeddings, axis=0)\n",
        "    normalized_avg1 = avg / np.linalg.norm(avg)\n",
        "    embeddings2 = model.encode(titles2, convert_to_numpy=True)\n",
        "    avg = np.mean(embeddings2, axis=0)\n",
        "    normalized_avg2 = avg / np.linalg.norm(avg)\n",
        "\n",
        "    similarity = np.dot(normalized_avg1, normalized_avg2)\n",
        "\n",
        "    similarityScores[(user1, user2)] = similarity\n",
        "\n",
        "print(\"Semantic Similarity Scores Between Users:\")\n",
        "for (user1, user2), score in list(similarityScores.items())[:10]:\n",
        "  print(f\"{user1} and {user2}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUdhX8kziLvF"
      },
      "outputs": [],
      "source": [
        "! pip install datasketch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocFKV6k2iK9a"
      },
      "outputs": [],
      "source": [
        "from datasketch import MinHash\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_minhash(titles):\n",
        "    m = MinHash(num_perm=128)\n",
        "    for title in titles:\n",
        "        for word in title.lower().split():\n",
        "            m.update(word.encode('utf8'))\n",
        "    return m\n",
        "\n",
        "minhashes = {user.name: get_minhash(convertIdtoNames(usersRecommendedTitles[user.name])) for user in platform.users}\n",
        "\n",
        "jaccardSimilarityScores = {}\n",
        "userNames = list(minhashes.keys())\n",
        "n = len(userNames)\n",
        "\n",
        "for i in range(n):\n",
        "    for j in range(i + 1, n):\n",
        "        user1 = userNames[i]\n",
        "        user2 = userNames[j]\n",
        "\n",
        "        minhash1 = minhashes[user1]\n",
        "        minhash2 = minhashes[user2]\n",
        "\n",
        "        sim = minhash1.jaccard(minhash2)\n",
        "\n",
        "        jaccardSimilarityScores[(user1, user2)] = sim\n",
        "\n",
        "print(\"\\nMinHash Jaccard Similarity Scores Between Users:\")\n",
        "for (user1, user2), score in jaccardSimilarityScores.items():\n",
        "    print(f\"{user1} and {user2}: {score:.4f}\")\n",
        "\n",
        "heatmap_data = pd.DataFrame(0.0, index=userNames, columns=userNames)\n",
        "\n",
        "for (user1, user2), score in jaccardSimilarityScores.items():\n",
        "    heatmap_data.loc[user1, user2] = score\n",
        "    heatmap_data.loc[user2, user1] = score\n",
        "\n",
        "plt.figure(figsize=(10, 8)) # Adjust figure size as needed\n",
        "sns.heatmap(heatmap_data, annot=True, cmap='Blues', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('MinHash Jaccard Similarity Heatmap Between Users')\n",
        "plt.xlabel('User')\n",
        "plt.ylabel('User')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGbkbSK3gAvL"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}